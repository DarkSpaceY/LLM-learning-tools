from typing import Dict, Any, Optional, AsyncGenerator, TYPE_CHECKING, List

if TYPE_CHECKING:
    from .tools.content_generator import Chapter, Section
import json
import asyncio
import aiohttp
from langchain.llms.base import LLM
from pydantic import Field
from pydantic_settings import BaseSettings
from .tools.content_generator import (
    generate_main_outline,
    generate_chapter_outline,
    generate_section_content
)


class OllamaConfig(BaseSettings):
    """Ollamaé…ç½®"""
    model_name: str = Field(
        default="deepseek-r1:8b",
        description="Ollamaæ¨¡å‹åç§°"
    )
    base_url: str = Field(
        default="http://localhost:11434",
        description="Ollama APIåŸºç¡€URL"
    )
    temperature: float = Field(
        default=0.7,
        description="ç”Ÿæˆæ¸©åº¦",
        ge=0.0,
        le=1.0
    )
    max_tokens: int = Field(
        default=1024,
        description="æœ€å¤§ç”Ÿæˆé•¿åº¦",
        gt=0
    )


class OllamaLLM(LLM):
    """è‡ªå®šä¹‰Ollama LLMç±»"""
    
    config: OllamaConfig = Field(default_factory=OllamaConfig)
    
    def __init__(
        self,
        model_name: str = "deepseek-r1:8b",
        base_url: str = "http://localhost:11434",
        temperature: float = 0.7,
        max_tokens: int = 1024
    ):
        """åˆå§‹åŒ–"""
        config = OllamaConfig(
            model_name=model_name,
            base_url=base_url,
            temperature=temperature,
            max_tokens=max_tokens
        )
        super().__init__(config=config)

    @property
    def _llm_type(self) -> str:
        """è¿”å›LLMç±»å‹"""
        return "ollama"

    @property
    def _identifying_params(self) -> Dict[str, Any]:
        """è¿”å›æ ‡è¯†å‚æ•°"""
        return {
            "model_name": self.config.model_name,
            "base_url": self.config.base_url,
            "temperature": self.config.temperature,
            "max_tokens": self.config.max_tokens
        }

    async def _call(
        self,
        prompt: str,
        **kwargs: Any
    ) -> AsyncGenerator[str, None]:
        """è°ƒç”¨Ollama APIç”Ÿæˆæ–‡æœ¬ï¼Œæ”¯æŒæµå¼è¾“å‡º"""
        async with aiohttp.ClientSession() as session:
            url = f"{self.config.base_url}/api/generate"
            headers = {"Content-Type": "application/json"}
            
            payload = {
                "model": self.config.model_name,
                "prompt": prompt,
                "stream": True,
                "options": {
                    "temperature": self.config.temperature,
                    # "num_predict": self.config.max_tokens,
                }
            }
            
            print(f"å‘é€è¯·æ±‚: {url}")
            print(f"ä½¿ç”¨æ¨¡å‹: {payload['model']}")

            async with session.post(
                url,
                json=payload,
                headers=headers
            ) as response:
                if response.status != 200:
                    error_text = await response.text()
                    raise ValueError(
                        f"APIé”™è¯¯ ({response.status}): {error_text}"
                    )

                async for line in response.content:
                    if not line:
                        continue
                        
                    text = line.decode('utf-8')
                    if text == "":
                        continue
                        
                    try:
                        data = json.loads(text)
                        if "error" in data:
                            raise RuntimeError(data["error"])
                        if "response" in data:
                            # ç›´æ¥è¿”å›åŸå§‹å“åº”
                            response = data["response"]
                            if response != "":  # ä»…ç”¨äºæ£€æŸ¥éç©º
                                yield response  # è¿”å›å®Œæ•´å“åº”ï¼ŒåŒ…å«æ¢è¡Œç¬¦
                    except json.JSONDecodeError:
                        print(f"æ— æ•ˆå“åº”: {text[:100]}")
                        continue


class ContentGenerator:
    """å†…å®¹ç”Ÿæˆå™¨ç±»"""

    def __init__(
        self,
        model_name: str = "deepseek-r1:8b",
        base_url: str = "http://localhost:11434",
        temperature: float = 0.7,
        max_tokens: int = 1024
    ):
        """åˆå§‹åŒ–"""
        self.llm = OllamaLLM(
            model_name=model_name,
            base_url=base_url,
            temperature=temperature,
            max_tokens=max_tokens
        )

    async def process_message(
        self,
        session_id: str,
        message: str,
        model: Optional[str] = None
    ) -> AsyncGenerator[str, None]:
        """å¤„ç†ç”¨æˆ·æ¶ˆæ¯å¹¶ç”Ÿæˆå†…å®¹"""
        # å¦‚æœæŒ‡å®šäº†æ¨¡å‹ï¼Œæ›´æ–°LLMé…ç½®
        if model:
            self.llm.config.model_name = model
            print(f"ä½¿ç”¨æ¨¡å‹: {model}")

        print(f"å¼€å§‹å¤„ç†ä¼šè¯ {session_id} çš„æ¶ˆæ¯")

        # æ£€æŸ¥OllamaæœåŠ¡æ˜¯å¦å¯ç”¨
        try:
            async with aiohttp.ClientSession() as session:
                try:
                    async with session.get(
                        f"{self.llm.config.base_url}/api/version",
                        timeout=5
                    ) as response:
                        if response.status != 200:
                            try:
                                error_body = await response.json()
                                error_msg = error_body.get(
                                    'error', 'æœåŠ¡å“åº”å¼‚å¸¸'
                                )
                            except (json.JSONDecodeError, aiohttp.ClientError):
                                error_msg = (
                                    await response.text() or 
                                    f"HTTP {response.status}"
                                )
                            raise RuntimeError(f"Ollamaå¼‚å¸¸: {error_msg}")
                            
                        version_info = await response.json()
                        version = version_info.get('version', 'unknown')
                        print(f"Ollamaç‰ˆæœ¬: {version}")
                except aiohttp.ClientError as e:
                    raise RuntimeError(f"æ— æ³•è¿æ¥Ollama: {e}")
                except asyncio.TimeoutError:
                    raise RuntimeError("è¿æ¥Ollamaè¶…æ—¶")
                    
        except Exception as e:
            error = str(e)
            print(f"\né”™è¯¯ï¼š{error}")
            yield "\nâŒ OllamaæœåŠ¡æ£€æŸ¥å¤±è´¥"
            if "æ— æ³•è¿æ¥" in error or "è¶…æ—¶" in error:
                yield "\nğŸ’¡ æç¤ºï¼šè¯·æ£€æŸ¥æœåŠ¡æ˜¯å¦å·²å¯åŠ¨"
            elif "å¼‚å¸¸" in error:
                yield "\nğŸ’¡ æç¤ºï¼šæœåŠ¡å¯èƒ½éœ€è¦é‡å¯"
            return

        # å¼€å§‹æ¸è¿›å¼ç”Ÿæˆ
        try:
            # 1. ç”Ÿæˆä¸»è¦ç« èŠ‚
            print("å¼€å§‹ç”Ÿæˆä¸»è¦ç« èŠ‚...")
            yield {
                "type": "progress",
                "stage": "chapters",
                "status": "start"
            }

            chapters: List['Chapter'] = []
            chapter_count = 0
            
            async for data in generate_main_outline(message, self.llm):
                if isinstance(data, dict):
                    if data['type'] == "chunk":
                        yield {
                            "type": "chunk",
                            "content": data["data"]
                        }
                    elif data['type'] == "chapters":
                        for chapter in data["data"]:
                            chapter_count += 1
                            chapters.append(chapter)
                            yield {
                                "type": "chapter",
                                "data": {
                                    "number": chapter.number,
                                    "title": chapter.title,
                                    "description": chapter.description
                                }
                            }

            if not chapters:
                yield {
                    "type": "error",
                    "message": "ç”Ÿæˆç« èŠ‚å¤±è´¥"
                }
                return

            yield {
                "type": "progress",
                "stage": "chapters",
                "status": "complete",
                "count": chapter_count
            }

            # 2. ç”Ÿæˆå°èŠ‚
            print("\nå¼€å§‹ç”Ÿæˆå°èŠ‚...")
            yield {
                "type": "progress",
                "stage": "sections",
                "status": "start"
            }

            section_count = 0
            for chapter in chapters:
                print(f"\nå¤„ç†ç« èŠ‚: {chapter.title}")
                chapter.sections = []

                async for data in generate_chapter_outline(message, chapter, self.llm):
                    if isinstance(data, dict):
                        if data['type'] == "chunk":
                            yield {
                                "type": "chunk",
                                "content": data["data"]
                            }
                        elif data['type'] == "sections":
                            for section in data["data"]:
                                section_count += 1
                                chapter.sections.append(section)
                                yield {
                                    "type": "section",
                                    "data": {
                                        "chapter": chapter.number,
                                        "number": section.number,
                                        "title": section.title,
                                        "description": section.description
                                    }
                                }

            yield {
                "type": "progress",
                "stage": "sections",
                "status": "complete",
                "count": section_count
            }

            # 3. ç”Ÿæˆè¯¦ç»†å†…å®¹
            print("\nå¼€å§‹ç”Ÿæˆè¯¦ç»†å†…å®¹...")
            yield {
                "type": "progress",
                "stage": "content",
                "status": "start"
            }

            content_count = 0
            for chapter in chapters:
                for section in chapter.sections:
                    print(f"\nç”Ÿæˆå†…å®¹: {section.title}")
                    section_content = []
                    
                    async for data in generate_section_content(message, section, self.llm):
                        if isinstance(data, dict):
                            if data['type'] == "chunk":
                                content_count += 1
                                # å‘é€chunkåˆ°å‰ç«¯
                                yield {
                                    "type": "chunk",
                                    "content": data["data"]
                                }
                                section_content.append(data["data"])
                            elif data['type'] == "content":
                                # å®Œæˆä¸€ä¸ªå°èŠ‚çš„å†…å®¹ç”Ÿæˆåå‘é€å®Œæ•´å†…å®¹
                                full_content = data["data"]
                                section.content = full_content
                                yield {
                                    "type": "content",
                                    "data": {
                                        "chapter": chapter.number,
                                        "section": section.number,
                                        "content": full_content
                                    }
                                }

            yield {
                "type": "progress",
                "stage": "content",
                "status": "complete",
                "count": content_count
            }

            print("\nå†…å®¹ç”Ÿæˆå®Œæˆï¼")
            yield {
                "type": "complete",
                "message": "å†…å®¹ç”Ÿæˆå®Œæˆ"
            }

        except Exception as e:
            error = str(e) or "æœªçŸ¥é”™è¯¯"
            print(f"\né”™è¯¯ï¼š{error}")
            yield {
                "type": "error",
                "message": f"ç”Ÿæˆè¿‡ç¨‹å‡ºé”™: {error}"
            }


# åˆ›å»ºä»£ç†å®ä¾‹
langchain_agent = ContentGenerator()